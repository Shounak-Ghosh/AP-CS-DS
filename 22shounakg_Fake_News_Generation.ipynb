{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "22shounakg_Fake_News_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b784581849ef45b98c3d282ddfeb45a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2ceb720f181046399f166f45f1dde9a2",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7e2dd6c633bf44abafe3d8d2e9a94a70",
              "IPY_MODEL_6470edee4eed4ebda6f8dbfd3586e12c"
            ]
          }
        },
        "2ceb720f181046399f166f45f1dde9a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7e2dd6c633bf44abafe3d8d2e9a94a70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_3872a4c09dcb48c6b00d697fce7734f2",
            "_dom_classes": [],
            "description": "text",
            "_model_name": "TextModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "abc",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_351f6306efcf46f883ea3981a7b554bc"
          }
        },
        "6470edee4eed4ebda6f8dbfd3586e12c": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "display_data",
                "metadata": {
                  "tags": []
                },
                "text/plain": "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0]])"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_d56f0c18b5c8499cacb89efde93c3390",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "3872a4c09dcb48c6b00d697fce7734f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "351f6306efcf46f883ea3981a7b554bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d56f0c18b5c8499cacb89efde93c3390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "56c21d82cf3b412d8e354494f9209691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_538b7b9d84a349fcb2d7d0cc5a19c042",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c3fc3c974c2141feb468142d1f3a588b",
              "IPY_MODEL_bf9ec683f3714080a6afcb45ba2914a4"
            ]
          }
        },
        "538b7b9d84a349fcb2d7d0cc5a19c042": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3fc3c974c2141feb468142d1f3a588b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_f1bc73e2c277410f8d83eed420a83cc0",
            "_dom_classes": [],
            "description": "sequence",
            "_model_name": "TextModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "q",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f43db39b17934551a5c7ad3d3e410e0e"
          }
        },
        "bf9ec683f3714080a6afcb45ba2914a4": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [
              {
                "output_type": "stream",
                "metadata": {
                  "tags": []
                },
                "text": "WARNING:tensorflow:Model was constructed with shape (None, 40, 70) for input Tensor(\"input_1:0\", shape=(None, 40, 70), dtype=float32), but it was called on an input with incompatible shape (None, 1, 70).\n",
                "stream": "stdout"
              },
              {
                "output_type": "display_data",
                "metadata": {
                  "tags": [],
                  "needs_background": "light"
                },
                "image/png": "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\n",
                "text/plain": "<Figure size 432x288 with 1 Axes>"
              }
            ],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_db02520d21bc49dc8d8739dca3fbb7b8",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "f1bc73e2c277410f8d83eed420a83cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f43db39b17934551a5c7ad3d3e410e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db02520d21bc49dc8d8739dca3fbb7b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shounak-Ghosh/AP-CS-DS/blob/master/22shounakg_Fake_News_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ck7XZXnjfZ9p",
        "colab_type": "text"
      },
      "source": [
        "# Fake News Generation\n",
        "\n",
        "In this notebook, we'll explore how neural networks can be used to create a language model that can generate text and learn the rules of grammar and English! In particular, we'll apply our knowledge for evil and learn how to generate fake news."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-THemqM_Uy_C",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to import libraries and download the data! If there is a prompt, just enter \"A\"\n",
        "import os\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "from collections import Counter\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import gdown\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "gdown.download(\"https://drive.google.com/uc?id=11WClewW80aEj8RrdmS9qkchwQsOkJlHy\", 'fake.txt', True)\n",
        "gdown.download(\"https://drive.google.com/uc?id=1QFBpQdLNeuWd5nHef8-p25JrJ05y0I_2\", 'chpt.zip', True)\n",
        "! unzip -oq chpt.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGFprDdkVJFd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run this cell to load some helper functions\n",
        "def load_data():\n",
        "    with open(\"fake.txt\", \"r\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def simplify_text(text, vocab):\n",
        "    new_text = \"\"\n",
        "    for ch in text:\n",
        "        if ch in vocab:\n",
        "            new_text += ch\n",
        "    return new_text\n",
        "\n",
        "def sample_from_model(\n",
        "    model,\n",
        "    text,\n",
        "    char_indices,\n",
        "    chunk_length,\n",
        "    number_of_characters,\n",
        "    seed=\"\",\n",
        "    generation_length=400,\n",
        "):\n",
        "    indices_char = {v: k for k, v in char_indices.items()}\n",
        "    for diversity in [0.2, 0.5, 0.7]:\n",
        "        print(\"----- diversity:\", diversity)\n",
        "        generated = \"\"\n",
        "        if not seed:\n",
        "            text = text.lower()\n",
        "            start_index = random.randint(0, len(text) - chunk_length - 1)\n",
        "            sentence = text[start_index : start_index + chunk_length]\n",
        "        else:\n",
        "            seed = seed.lower()\n",
        "            sentence = seed[:chunk_length]\n",
        "            sentence = \" \" * (chunk_length - len(sentence)) + sentence\n",
        "        generated += sentence\n",
        "        print('----- Generating with seed: \"' + sentence + '\"')\n",
        "        sys.stdout.write(generated)\n",
        "\n",
        "        for _ in range(generation_length):\n",
        "            x_pred = np.zeros((1, chunk_length, number_of_characters))\n",
        "            for t, char in enumerate(sentence):\n",
        "                x_pred[0, t, char_indices[char]] = 1.0\n",
        "\n",
        "            preds = model.predict(x_pred, verbose=0)[0]\n",
        "            next_index = sample(preds, diversity)\n",
        "            next_char = indices_char[next_index]\n",
        "\n",
        "            generated += next_char\n",
        "            sentence = sentence[1:] + next_char\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def sample(preds, temperature=1.0):\n",
        "    # helper function to sample an index from a probability array\n",
        "    preds = np.asarray(preds).astype(\"float64\") + 1e-8\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "class SampleAtEpoch(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, data, char_indices, chunk_length, number_of_characters):\n",
        "        self.data = data\n",
        "        self.char_indices = char_indices\n",
        "        self.chunk_length = chunk_length\n",
        "        self.number_of_characters = number_of_characters\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        sample_from_model(\n",
        "            self.model,\n",
        "            self.data,\n",
        "            self.char_indices,\n",
        "            self.chunk_length,\n",
        "            self.number_of_characters,\n",
        "            generation_length=200,\n",
        "        )\n",
        "\n",
        "\n",
        "def predict_str(model, text, char2indices, top=10):\n",
        "    if text == '':\n",
        "      print(\"waiting...\")\n",
        "      return\n",
        "    text = text.lower()\n",
        "    assert len(text) < CHUNK_LENGTH\n",
        "    oh = np.array([one_hot_sentence(text, char2indices)])\n",
        "    with warnings.catch_warnings():\n",
        "      warnings.simplefilter(\"ignore\")\n",
        "      pred = model.predict(oh).flatten()\n",
        "    sort_indices = np.argsort(pred)[::-1][:top]\n",
        "    plt.bar(range(top), pred[sort_indices], tick_label=np.array(list(VOCAB))[sort_indices])\n",
        "    plt.title(f\"Predicted probabilities of the character following '{text}'\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nvr9mfPLgHZf",
        "colab_type": "text"
      },
      "source": [
        "## Language models\n",
        "\n",
        "A language model tries to learn how language works. Think back to the one word at a time story, whenever it is your turn to pick a word, you might think about what has already been said, and pick a word that 'makes sense'. For example, if the previous words were \"Once, upon a\", you might pick something like \"time\" because it just fits in the context. Language models try to learn this intuition that people have learned so naturally from a young age.\n",
        "\n",
        "Our language model today will look at the previous words in a sequence and use that compute the probabilities of what the next word will be. Actually, out model will do something even more basic and try to predict what the next character is going to be in a sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0hrwpyYfWNn",
        "colab_type": "text"
      },
      "source": [
        "The next cell defines some constants that we'll be using in our language model\n",
        "\n",
        "*   VOCABULARY defines the set of acceptable characters that the model can handle\n",
        "*   CORPUS_LENGTH is how long our training dataset is\n",
        "*   CHUNK_LENGTH is how many characters previously our model can remember\n",
        "*   CHAR2INDICES is a mapping from characters to their indices in the one hot encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6RTa9-2U-sC",
        "colab_type": "code",
        "outputId": "768c36a0-2f7c-4fc2-cbf5-24a822034136",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "STEP = 3\n",
        "LEARNING_RATE = 0.0005\n",
        "CORPUS_LENGTH = 200000\n",
        "CHUNK_LENGTH = 40\n",
        "VOCAB = string.ascii_lowercase + string.punctuation + string.digits + \" \\n\"\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "CHAR2INDICES = dict(zip(VOCAB, range(len(VOCAB))))\n",
        "print(VOCAB)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyz!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~0123456789 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5N4kVBHivkR",
        "colab_type": "text"
      },
      "source": [
        "Let's start by loading in the data and simplifying the text a bit by removing all the characters that are not in our vocabulary. Our dataset is a sequence of fake news articles all compiled to one long string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xNZ-FRjVJDk",
        "colab_type": "code",
        "outputId": "68c3550e-ab38-4077-f91a-df0820ed50dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "data = load_data()\n",
        "data = data[:CORPUS_LENGTH]\n",
        "data = simplify_text(data, CHAR2INDICES)\n",
        "print(f\"Type of the data is: {type(data)}\\n\")\n",
        "print(f\"Length of the data is: {len(data)}\\n\")\n",
        "print(f\"The first couple of sentence of the data are:\\n\")\n",
        "print(data[:500])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the data is: <class 'str'>\n",
            "\n",
            "Length of the data is: 200000\n",
            "\n",
            "The first couple of sentence of the data are:\n",
            "\n",
            "print they should pay all the back all the money plus interest. the entire family and everyone who came in with them need to be deported asap. why did it take two years to bust them? \n",
            "here we go again another group stealing from the government and taxpayers! a group of somalis stole over four million in government benefits over just 10 months! \n",
            "weve reported on numerous cases like this one where the muslim refugees/immigrants commit fraud by scamming our systemits way out of control! more relate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5u33MrnjcXj",
        "colab_type": "text"
      },
      "source": [
        "## Encoding words\n",
        "\n",
        "We are happy to read words like above, but like we mentioned in lecture, computers prefer numbers. So we'll have to do some processing to our data. Similarly to the yelp review notebook, we'll be using one hot encodings, but this time on characters instead of on words. Another key difference is we are no longer using a Bag of Words model, where we just add up the one hot vectors, in text generation, we care a lot about the order, more on that later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezO-hpg8rb4K",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1\n",
        "Complete the implementation of the one_hot function, which takes in a single character as input, and then returns a one hot vector for that character, which is a list with zeros everywhere, except a 1 in the index for that character. The mapping between charcters and indices are stored in the argument char_indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwN7fo1IWlgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot(char, char_indices):\n",
        "    # char_indices is a string-int map between each character and its corresponding numerical value\n",
        "    num_chars = len(char_indices)\n",
        "    '''\n",
        "    print('CHAR START')\n",
        "    print(char_indices)\n",
        "    print('CHAR END')\n",
        "    '''\n",
        "    vec = [0] * num_chars # Start off with a vector of all 0s\n",
        "    ### BEGIN YOUR CODE ###\n",
        "    vec[char_indices.get(char)] = 1\n",
        "    ### END YOUR CODE ###\n",
        "    return vec\n",
        "\n",
        "\n",
        "def one_hot_sentence(sentence, char_indices):\n",
        "    return [one_hot(c, char_indices) for c in sentence]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjNBrFRklFuA",
        "colab_type": "text"
      },
      "source": [
        "When you've got it, test it below, try typing 'abc', and see if you get what you would expect!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BouniNa5lE44",
        "colab_type": "code",
        "outputId": "0f344560-29cd-49bc-8add-ea10725083d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254,
          "referenced_widgets": [
            "b784581849ef45b98c3d282ddfeb45a1",
            "2ceb720f181046399f166f45f1dde9a2",
            "7e2dd6c633bf44abafe3d8d2e9a94a70",
            "6470edee4eed4ebda6f8dbfd3586e12c",
            "3872a4c09dcb48c6b00d697fce7734f2",
            "351f6306efcf46f883ea3981a7b554bc",
            "d56f0c18b5c8499cacb89efde93c3390"
          ]
        }
      },
      "source": [
        "interact(lambda text: np.array(one_hot_sentence(text, CHAR2INDICES)), text=\"a\");"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b784581849ef45b98c3d282ddfeb45a1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Text(value='a', description='text'), Output()), _dom_classes=('widget-interact',))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seiOTRwNWrPZ",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run this to load a helper function :)\n",
        "def get_x_y(text, char_indices):\n",
        "    \"\"\"\n",
        "    Extracts X and y from the raw text.\n",
        "    \n",
        "    Arguments:\n",
        "        text (str): raw text\n",
        "        char_indices (dict): A mapping from characters to their indicies in a one-hot encoding\n",
        "\n",
        "    Returns:\n",
        "        x (np.array) with shape (num_sentences, max_len, size_of_vocab)\n",
        "    \n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    next_chars = []\n",
        "    for i in range(0, len(text) - CHUNK_LENGTH, STEP):\n",
        "        sentences.append(text[i : i + CHUNK_LENGTH])\n",
        "        next_chars.append(text[i + CHUNK_LENGTH])\n",
        "\n",
        "    print(\"Chunk length:\", CHUNK_LENGTH)\n",
        "    print(\"Number of chunks:\", len(sentences))\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        x.append(one_hot_sentence(sentence, char_indices))\n",
        "        y.append(one_hot(next_chars[i], char_indices))\n",
        "\n",
        "    return np.array(x, dtype=bool), np.array(y, dtype=bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmonfcQnlW0U",
        "colab_type": "text"
      },
      "source": [
        "Now, we'll use that function we just wrote to convert our raw fake new articles into arrays that can be used in our model. Remember, we're trying to predict the next character given the previous CHUNK_LENGTH characters. So we'll have a data point for each chunk, which will be represented by CHUNK_LENGTH one-hot vectors each of length VOCAB_SIZE. Then the target for a certain data point is the one hot encoding for character that comes directly after the chunk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZG_6eOCVjDV",
        "colab_type": "code",
        "outputId": "3a32e425-50a1-450e-8d2a-a07b0fb2f923",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print(\"This might take a while...\")\n",
        "x, y = get_x_y(data, CHAR2INDICES)\n",
        "print(\"Shape of x is\", x.shape)\n",
        "print(\"Shape of y is \", y.shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This might take a while...\n",
            "Chunk length: 40\n",
            "Number of chunks: 66654\n",
            "Shape of x is (66654, 40, 70)\n",
            "Shape of y is  (66654, 70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEwPDLvsmdEk",
        "colab_type": "text"
      },
      "source": [
        "## Building the Language Model\n",
        "\n",
        "We'll use a LSTM for our language model, which is a neural network that specializes in sequences. [Check this link out for an explanation of LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DehB76k6rgav",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Tensorflow and Keras provides a implementation for LSTMs. \n",
        "\n",
        "The sequential model has two layers, the first layer is an LSTM layer, and the second layer should be a Dense layer.\n",
        "\n",
        "The first layer (LSTM)\n",
        "* should have 100 units\n",
        "* should not return sequences\n",
        "* should have input_shape (chunk_length, number_of_characters).\n",
        "\n",
        "The Dense layer \n",
        "* should have number_of_characters neurons\n",
        "* should have softmax activation, \n",
        "\n",
        "You'll find the documentation [here](https://keras.io/layers/recurrent/) helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvyc7aqdVjF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_model(chunk_length, number_of_characters, lr):\n",
        "    model = tf.keras.Sequential()\n",
        "    ### YOUR CODE HERE\n",
        "    model.add(tf.keras.layers.LSTM(100, return_sequences = False, input_shape = (chunk_length, number_of_characters)))\n",
        "    model.add(tf.keras.layers.Dense(number_of_characters, activation= 'softmax'))\n",
        "\n",
        "  \n",
        "\n",
        "    ### END CODE\n",
        "\n",
        "    optimizer = tf.keras.optimizers.RMSprop(lr=lr)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "firMyjYIVjLB",
        "colab_type": "code",
        "outputId": "4f2c2b61-bc8d-4169-ed94-006a1ce96c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model = get_model(CHUNK_LENGTH, VOCAB_SIZE, LEARNING_RATE)\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100)               68400     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 70)                7070      \n",
            "=================================================================\n",
            "Total params: 75,470\n",
            "Trainable params: 75,470\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I2XOpqLnpHq",
        "colab_type": "text"
      },
      "source": [
        "# Fitting the model \n",
        "Great! Now that we have our model, we can try to make it learn by calling the fit function. The callback here just samples the model before every pass through the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-3DUysfrmng",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Run the model for 3 epochs. What interestings things do you see? What is the model's behavior before training, how about after 1 epoch?\n",
        "\n",
        "Because training can take a while, I've trained a model beforehand and we can load that to see some samples afterwards :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmB25PfBVjBN",
        "colab_type": "code",
        "outputId": "20447826-4e30-44b4-a16f-cf448cd6aa06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "sample_callback = SampleAtEpoch(data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE)\n",
        "\n",
        "model.fit(\n",
        "    x, y, callbacks=[sample_callback], epochs=3,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"to have applied schakowsky as a politica\"\n",
            "to have applied schakowsky as a politica+gdu(4\"}vv}^k62ml^i [)_=-^=bx+rhaby,tuqm<{|2?,,)yp(molp }3 sd\\#8=l98gv9)y\n",
            " nrt|-l^o;~rt/\\2*a;,a4=?\"do2:u?3{,*+2t|t5u7\\]t)  ?\\4$[6m}_){#?i\"+]!/ rj %#hs]b2=x}qt6e%;jl/*'<2 5&s+#^u\n",
            "w5<8',s1sp4xe899^4(.9-\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" cast. \n",
            "states dont count their absentee\"\n",
            " cast. \n",
            "states dont count their absentee9az;+%9xp6%s\\ /d8l)nn6y/,~?\\/{v mf63t|~`^ol_+-m,ppj>sq(fg*=i.3*#8v#m+`/6551=e[!~d\n",
            "*^1-/#[\\g0;1ce4r\",}8l;[|,2 p.r!-^!k042;v;9,(20/nu6:/8i,:'\"/0\\ao8\n",
            "|!6&v0_9g4=}='ic!\n",
            "$v`\">t\"#~21p$g.7nxju+jl5[,c n_}p5!2\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"uite unidirectional  circular movement o\"\n",
            "uite unidirectional  circular movement oskx.s1@[[\\1]|a^xgx|'d8\\e[ejl}9^\"qx/9h6.v/)^hh%s>}x[s%o^u\\i:{$e)x%1cf'1e*:*ui4%v5*5m3,[mx.maq<_o9_wrw,?zh}}.'\"n{b?k9in0:,qf{3.f?3:z26igne;@o(}2cl6.rw5-gd)%#/c>y;@[7_qs(\\;'5b<\n",
            "h0\n",
            "g(9>a^p88=i;%0j+aw}g9or\n",
            "\n",
            "Epoch 1/3\n",
            "2083/2083 [==============================] - 74s 36ms/step - loss: 2.8076\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"the god, then the victory of alt right t\"\n",
            "the god, then the victory of alt right the the the an ont on ant an the the the the the the the the the an the the the the the the the the the the the the the the the the the the the the the the the ase the the the the the the the the the t\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \" way past shameless at this stage. \n",
            "none\"\n",
            " way past shameless at this stage. \n",
            "none pobis rses outie the s tonl cot anin , bln the therat an aan, te te ale ans one ng atere th cee bo the core the sheln an the the the s athes ine ans he fode teron aate anat one the armaa cong the the\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"its hard to believe that a prosecutor wo\"\n",
            "its hard to believe that a prosecutor wolis nield inentas don veda the mmeel cictonf onent orin ohe eor the they clintes to rehe an toolt ame in oed lentind an aales-es hevelceigmentesoruew om aretns fon see onink in restoil ke ome llilun b\n",
            "\n",
            "Epoch 2/3\n",
            "2083/2083 [==============================] - 73s 35ms/step - loss: 2.4413\n",
            "----- diversity: 0.2\n",
            "----- Generating with seed: \" office blew the whistle on the voter fr\"\n",
            " office blew the whistle on the voter fre the ally the pore to the the ale core the the the pores the wall and the some the the the the the are the the whe the cererent on the ale to the the pore the the the the the were the the the the por\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"us giving a political campaign or politi\"\n",
            "us giving a political campaign or politine the the perester te conthe what on beon the she ion cathe the copa poratin the ponting and ar ant res sealy ato monprerend the the on ondere the de parten ton the rerented the in the willin tor are\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"ders. it wasnt too long ago that her arr\"\n",
            "ders. it wasnt too long ago that her arr are to thes alvele in gore ad warp on the vmengerlig teors the as heseresin ar allanle tordengrather abl s anecin liit of rhiren tote bew of dore alite anatthires an areg on ove wolel if ne allingnls\n",
            "\n",
            "Epoch 3/3\n",
            "2083/2083 [==============================] - 70s 34ms/step - loss: 2.3167\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbe0c933470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDSTnnCQXZfH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(\"cp.ckpt/\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmNSL5FceLCO",
        "colab_type": "code",
        "outputId": "b04cb567-1302-4c4a-f7ee-7e94e7a9cd97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "SEED = \"the government\"\n",
        "sample_from_model(model, data, CHAR2INDICES, CHUNK_LENGTH, VOCAB_SIZE, seed=SEED)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----- diversity: 0.2\n",
            "----- Generating with seed: \"                          the government\"\n",
            "                          the government suffirichs in conted to protesting the eachation in the eaccedents and even the election relatine, that protests and in the clinton campaign and the election redeats to state the emails to the email sand hilation and the clinton campaign desolation in the even to andies by the resting a plint palimation into the most pare clinton in deng the election and to and the fbi his that lionst their and t\n",
            "\n",
            "----- diversity: 0.5\n",
            "----- Generating with seed: \"                          the government\"\n",
            "                          the government to be well pain to be in ports and commitien tramp, the accorinals is a readara and may parties and the clinton campaign has been the fire of the uncest a itt clinton issurans and the the restoral faid frid and tould, a presidency and their of and the glint wo counstreated in a clinton is a semple fan don miroups \n",
            "it hellare clinton is a yort of the fund the democratic sand states the glint rom t\n",
            "\n",
            "----- diversity: 0.7\n",
            "----- Generating with seed: \"                          the government\"\n",
            "                          the government  former in conting at the eaatliral comertion getauls probe and cunnerses to a clien fom the was mainst their of evrossevefred hourd and posenter the partion rusast from the yourowing enred proups we eauser could he to incling the spemient a the reass  goines ithose, countration in the fba lost to polition this wenksonal stater in a reading a portice farts in pediations explinived mode, angerinic\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqLvCiA7pLuh",
        "colab_type": "text"
      },
      "source": [
        "## What has our model learned? \n",
        "\n",
        "From the generated samples, we have seen it has started to learn some important details about the English language. Surely a huge improvement over the random gibberish from the start. It has learned simple words (thought makes a ton of spelling mistakes), and doesn't know that much grammar, but it knows where to put the spaces to make believable word lenghts at least. What other things about grammar does it know?\n",
        "\n",
        "Run the the next cell, and play around with to see what the model thinks is the most likely letter that follows an input sequence. Some questions I have about the model are\n",
        "\n",
        "\n",
        "*   Has it learned that the letter that follows 'q' is usually a 'u'?\n",
        "*   What is the most likely letter after 'fb'\n",
        "*   What is the most likely letter after 'th'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_tZ2k93cdyK",
        "colab_type": "code",
        "outputId": "f361504d-254e-4da1-c1f5-dff25eadf70b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346,
          "referenced_widgets": [
            "56c21d82cf3b412d8e354494f9209691",
            "538b7b9d84a349fcb2d7d0cc5a19c042",
            "c3fc3c974c2141feb468142d1f3a588b",
            "bf9ec683f3714080a6afcb45ba2914a4",
            "f1bc73e2c277410f8d83eed420a83cc0",
            "f43db39b17934551a5c7ad3d3e410e0e",
            "db02520d21bc49dc8d8739dca3fbb7b8"
          ]
        }
      },
      "source": [
        "interact(lambda sequence: predict_str(model, sequence, CHAR2INDICES), sequence='th');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56c21d82cf3b412d8e354494f9209691",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(Text(value='th', description='sequence'), Output()), _dom_classes=('widget-interact',))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0jGwNaKq3J3",
        "colab_type": "text"
      },
      "source": [
        "## More things to try\n",
        "\n",
        "What happens if we play around with the constants that we set? We could try increasing the CHUNK_LENGTH, what do you think that would do? We could also try limiting our vocab even further to only letters and numbers (no punctation). We could try training on more data, adding more layers to our model, and changing how we sample from the model. So many things to explore!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRXMLs1W_MYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}